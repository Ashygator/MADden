\documentclass[11pt]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{fullpage}

\usepackage{xspace}
\usepackage{graphicx}
\usepackage{color}
\usepackage{geometry}
\newcommand{\system}{MADden\xspace}

\newcommand{\ceg}[1]{{\textcolor{green}{#1 -- CEG}}}

\title{MADden : In-Database Text Analytics}
\author{Morgan Bauer 9890-4838 \\
  Christan Grant 8143-3970 \\
  Joir-dan Gumbs 6148-9357}
\date{December 13, 2011}
\begin{document}
\maketitle

\begin{figure}
  \begin{center}
    \includegraphics[width=104mm]{logo.jpg}
   %\includegraphics[width=4cm]{logo.jpg}
    \caption{Logo for overarching project.}
    \label{fig:logo}
  \end{center}
\end{figure}


\begin{abstract}
In many domains, structured data and unstructured text are
both important natural resources to fuel data analysis. Statistical
text analysis needs to be performed over the text data to extract
structured information for further query processing. Common practice
in industry requires developers to connect multiple tools to build
off-line batch processes to perform such analysis tasks. \system is
an integrated system for ad hoc real-time query processing over
structured and unstructured data. \system is built on top of MADlib,
and PostgreSQL/Greenplum, and integrates in-database text extraction
techniques from BayesStore. This demonstration uses two application
domains---computation journalism and political campaign
management---to show (1) real-time processing of ad hoc queries
involving statistical text analysis; (2) joining between structured
and textual data; and (3) query-time rendering of
visualizations over the query result.
\end{abstract}




  \section{Introduction}

  \textbf{\system, MAD, Magnetic Agile Deep.}
  Mad Libs is a classic fill in the blank game that usually have amusing and unforeseen outcomes.
  In the \system project, our initial problem domain deals with football,
  and John Madden is a hall of fame coach, former player, and former commentator.
	He is most popular
  for the long-running series of video games that bear his name.

  \begin{enumerate}
  \item The problem statement and a summary of the main contributions and results of the project.

    There are large amounts of structured and unstructured data that is freely available,
		we create a proof of concept system to show how to intermix the two.
    We use statistical text analysis techniques to extract structured information from unstructured 
		data, and pass this on to later stages for further processing.

    We provide the ability to run queries over heterogeneous data such as structured statistical 
		fact tables and unstructured natural language.

    Additionally, we  scrape dirty data and turn it into clean information, and then presenting 
		the information in an easy to digest form.

    We want to create queries over sports data to gain insight of a player and/or teams
		performance along with fan sentiment.

    We do however hope to be generic, and liberal in the data we take in, so that the same techniques can be applied to multiple domains.

    There is a web interface that provides pre-created web form queries that can be filled in 
		like the Mad Libs game.
    It serves as a proving ground for various queries we test and run.
		Additionally, we create visualization of the results in html and using charts.

  \item What is the responsibility of each member of the group?


   \textbf{Morgan} did consulting work among the various group members, and between groups.
    Official NFL team blogs. NFL stats from cbssports.com. He also led in the creation of the paper.
		
		\textbf{Christan} built the \system website -- including the Mad lib style interface and the 
		result page with the charts and graphs. He obtained the tweet corpus, created the functionality
		for entity resolution, information extraction, and sentiment analysis. He also helped Morgan
		write the final paper.

		\textbf{Joir-dan} Joir-dan obtained play-by-play data from ESPN.com, CBS sports, blogs,
		and news recap articles.
		He also came up with queries for the web site.

  \end{enumerate}

  \section{Background}
  \subsection{Domain}

  Our initial domain is a sports domain and more specifically the National Football League (NFL).
  This domain is relevant for to sports journalist to do computational journalism.
  Covering all 32 teams with more than 1700 players is a time consuming task.
  A sports journalist would need a system
  that can analyze both the
  structured statistics (e.g., scores, biographic data) of teams and players and the
  unstructured tweets, blogs, and news about the games.
	Additionally, a coaches and scouts would find use in creating statistical queries over NFL data.



  \subsection{Background Information}



    Text analysis uses the state-of-the-art statistical machine learning (SML) methods to extract structured information,
    such as entities, relations, sentiments, topics, from text.

    The result of the text analysis can be joined with other
    structured data sources to perform analysis. For example, the sports
    journalist may want to join the sentiment result from tweets and the
    scores of the Arizona Cardinals\footnote{http://www.azcardinals.com}
    (NFL Team) to see their correlations.
		Below we discuss the algorithms we use.

  \begin{enumerate}

  \item Part of Speech (POS) Tagging

	We used part of speech taking from the natural language toolkit (NLTK). 
	The calls for POS tagging was wrapped into a Postgres user-defined function.


  \item Sentiment

    Sentiment analysis extracts the polarity of subjective opinions from documents.
    These subjective feelings are important for assessing the popularity of subjects,
    and can be used to establish the collective feelings of the public on a subject.


  \item Information Extraction (IE) 

	IE is the concept of identifying objects of interest,
    using relations present in the data.
    This varies from simple relations between subjects,
    to recognizing specific entities within text.
		For example, we may want to extract all the `organizations' mentioned in a 
		a news article.


  \item Entity Resolution (ER) 
	
		ER is the processing of recognizing and disambiguating different subjects in a document.
    That is, recognizing that people are present,
    or mentioned, and knowing who they are.

    Where as IE was concerned about the existence of various objects,
    and identifying their existence,
    ER is concerned with taking these entities and knowing whether they are the same or different.
		In may situations, the output of information extraction becomes the input 
		to an entity resolution task.


  \item Conditional Random Fields (CRF) are a type of graphical model.
    A CRF is a supervised learning algorithm, requiring labeled training examples.
    In our case we are using CRFs to label sequences of natural language text.
    The quality and breadth of this set determines how well the trained CRF performs.
    They can be used for different things depending on their training set.
    We intended to use CRFs for POS tagging, IE, and ER.

     \end{enumerate}
  \subsection{Related work}

Several similar systems have been created by companies for 
internal and specific tasks
As far as we know, \system is the only system to integrate as much text analytic
tasks into a single query interface allowing the ability to 
create ad hoc queries, as if they fit.
  This is a ``what we want, not how to get it'' style query.

In computational journalism, the panda project\footnote{http://blog.apps.chicagotribune.com/}  
is a new effort to allows journalists to search and find relationship between 
many news stories. However, their system does not provide the same type of 
query interface.
The Tweelq project was created at MIT and it allowed a query interface for 
sentiment analysis over tweets. They created a parser for SQL and 
sampled the live twitter stream.

Daisy Wang \cite{wang2011hybrid} developed in-database techniques for 
conditional random fields. Our system aim to also have a completely in-database
text analytics stack.


  \section{System Description (4 pages)}
  \subsection{System Architecture}
  %What is the final system architecture?

  {\system} is a four layered system, as can be seen in Figure \ref{fig:arch}.
  The user interface is where both naive and advanced users can construct queries over text, structured data, and models.
  From the user interface,
  queries are then passed to the DBMS,
  where MADLib \footnote{http://madlib.net/} and {\system} libraries sit on top of the query processor to add statistical and text processing functionality.
  % These queries are processed using PostgreSQL/Greenplum's Parallel DB architecture to further optimize on replicated storage.


  \subsection{System Components}
  %What are the system components and statistical methods developed?

  \system uses PostgreSQL, with MADlib overtop.
  We hope to scale up and use Greenplum massively parallel processing database.

  There is a new feature for MADlib that implements CRF evaluation, but not 
	training, in the database.
  The full text search capabilities of PostgreSQL are leveraged,
  as is the technique of inverted indexing.

	In addition, we developed our own text analytic functions, these are
	described below.

    \subsubsection{Information Extraction}

    Once we have the text documents, we would like to then be able to identify
    objects of interest. Information Extraction methods become increasingly
    important as the size of our corpus gets large. Many methods exist for in-database data
    extraction, but we are currently focused on building upon existing work in
    utilizing Conditional Random Fields (CRFs) for query-time extraction of
    entities.

    This is applied to the unstructured twitter data,
    and the semi-structured play-by-play data.
    As well, this would be applied for Named Entity Recognition purposes to general text, for identifying players and teams.
		Presently, we rely on models built by nltk to perform part-of-speech 
		tagging and subsequently information extraction. NLTK can label 
		terms and tag them with the label type. 


    \subsubsection{Entity Resolution}
    \label{sec:SCER}

    ER links various forms and representations of the same background object,
    so that a query for one of them returns the results for all of them.
    This is necessary for two reasons,
    the informality of speaking in documents like blog posts and twitter tweets,
    which leads to misspellings and slang
    and the more  general case of people or companies having many names to refer to them.

    An example of the informality of the slang \& misspellings in twitter is
		``tmrw'' for ``tomorrow'' and ``NY Jest'' vs ``NY Jets''. Sometime 
		misspellings are intensional.

    An example of a person having multiple names is the
    nickname for Larry Fitzgerald which is ``Sticky Fingers''.
		Companies may also have acronym and nickname
    for example ``International Business Machines'' vs ``IBM'' and ``Big Blue''.
    and ``New York City'' vs ``Big Apple''.

    An example of multiple things with one name is
    Arizona Cardinals vs cardinals as a bird,
    Miami Dolphins vs dolphins as a fish and dolphins as a mammal.


    This is implemented through q-grams for misspellings and alternate spellings,
    and an alias table for multiple names of an entity.
    q-grams is the automatic approach that does fuzzy string matching,
    with a threshold value that can be varied.
    The alias table being hand constructed.


    \subsubsection{Q-grams}
    \label{qgram}

		Qgrams also known as character ngrams is implemented inside of the database.
		We can run a window function over strings for extract qgrams.
		Comparing the created qgrams of strings gives use an approximate string
		matching technique.
		We are implement it in pure database so it works well with MPP systems.
		Given a string \textcolor{blue}{Tim Tebow} we can create a 3-gram by 
		using a sliding window.
		
		\begin{tabular}{|l|l|}
		\hline
		\textbf{Position} & \textbf{3-gram} \\
		\hline
		$1$ & \#\#T \\
		\hline
		$2$ & \#TI \\
		\hline
		$3$ & TIM \\
		\hline
		$4$ & IM\_ \\
		\hline
		$5$ & M\_T \\
		\hline
		$6$ & \_TE \\
		\hline
		$7$ & TEB \\
		\hline
		$8$ & EBO \\
		\hline
		$9$ & BOW \\
		\hline
		$10$ & OW\% \\
		\hline
		$11$ & W\%\% \\
		\hline
		\end{tabular}

Given two tables we can compare the overlap of two qgrams and compute a
similarity. The we compute the similarity using the equation
$$ S(table1,table2) = \frac{|table1 \cap table2|}{\min\{|table1|, |table2|\}}$$

Qgram approach may fail with small words with error in the center. An 
example of this is displayed in the VOTKA vs VODKA example below.

    \begin{tabular}{|c|c|}
		\hline
      \textbf{VOTKA} & \textbf{VODKA} \\ \hline
      {\color{blue}\#\#V} & {\color{blue}\#\#V} \\
		\hline
      {\color{blue}\#VO} & {\color{blue}\#VO} \\
		\hline
      {\color{red}VOT} &  {\color{red}VOD} \\
		\hline
      {\color{red} OTK} &{\color{red} ODK} \\
		\hline
      {\color{red} TKA} &{\color{red} DKA} \\
		\hline
      {\color{blue}KA\$} & {\color{blue}KA\$} \\
		\hline
      {\color{blue}A\%\%} &  {\color{blue}A\%\%} \\
		\hline
    \end{tabular}

In general the qgram approach is very approximate and loses accuracy with
large documents.
Run time for search is $O(|T|*|P|)$ where $|T|$ and $|P|$ is the text and
search pattern.\footnote{For large documents preprocessing tricks are
available}


    \subsubsection{Sentiment Analysis}
    \label{sec:SCSA}

    Sentiment analysis extracts the polarity of subjective opinions from documents.
    The current implementation returns sentiment as a trinary option of positive, neutral, or negative.
    We would like to have more gradations in the extremeness of the opinion,
    as well as analysis of an opinion pertaining to a certain subject, or entity, that has been extracted.

    Currently, this is handled as a UDF which makes an out of database call across the network.
    It also does this on a document by document basis,
    when the submission API supports batches of documents\footnote{https://sites.google.com/site/twittersentimenthelp/}.
    Priority should be given to establish batch submissions,
    followed by writing the UDF as a local function,
    and possibly an in-database function.

    \subsubsection{Part of Speech Tagging}
    Part of Speech Tagging (POS tagging) labels each token of a sequence with a tag based on the grammar of a natural language.
    These tags are parts of speech, based on context and word definitions.
    The generators for these tags can come from both supervised and unsupervised sources.
    Statistical methods are used to generate taggers specific to a corpus, and thus a particular subject.
    These parts of speech are useful in other stages, such as entity resolution and information extraction (ER \& IR).

    
    The model was initially trained on email headers.
    This data was not generic enough to be useful in our tweet and news corpus.
    The individual documents were to small and not of a form similar enough to ensure correct tags.

    The next CRF was trained using email contents.
    While it had a higher accuracy, the training corpus was still too
		small at only 100 or 200 documents.

    We need to train a CRF specific to our domain,
    and would like to train a CRF specific to each type of text,
    such as a twitter CRF, a blog CRF, etc.

  \begin{figure}
    \begin{center}
      \includegraphics[width=104mm]{architecture-1.png}
      \caption{Original System Architecture}
      \label{fig:architecture}
    \end{center}
  \end{figure}

  \begin{figure}
    \begin{center}
      \includegraphics[scale=0.4]{arch.png}
      \caption{{Our updated \system} architecture}
      \label{fig:arch}
    \end{center}
  \end{figure}


  \begin{figure}
    \begin{center}
      \includegraphics[scale=0.3]{altachitecture.png}
      \caption{Overall {\system} architecture}
      \label{fig:altarch}
    \end{center}
  \end{figure}


  %\begin{figure}
  %  \begin{center}
  %    \includegraphics[scale=0.3]{altarchitecture2.jpg}
  %    \caption{{\system} architecture 2}
  %    \label{fig:altarch2}
  %  \end{center}
  %\end{figure}



  \subsection{Data Products}

  We don't have particular products, so much as abilities to deliver varying products, and adapt to the data as it changes.


	We created a set of tools to query over data and extract knowledge.
	We created a website to interface with the database and visualize results.


	We developed 5  Mad Lib style queries including;
	\begin{enumerate}
	\item Give me [number] [+/-/o] sentiment comments about [Player].
	\item Compare [Player1][Player2] by the twitter sentiment	over dates from
	[date one] and [data two] and return [number] results.
	\item Give me the [+/-/o] sentiment for the top [number] 
	[running back/quarter back/ wide receiver] for tweets.
	\item Return all the named entity tags from the text [text].
	\item Give me th [number] tweets and their sentiment corresponding to 
	one of the top [number]  [running back/quarter back/ wide receiver].
	\end{enumerate}


  \begin{figure}
    \begin{center}
      \includegraphics[scale=0.3]{web-ui.png}
      \caption{{\system} Web UI}
      \label{fig:web-ui}
    \end{center}
  \end{figure}

  \section{Experiments (2 pages)}
  \begin{enumerate}
    \item Data

		Twitter -- unstructured, microblogs, very small document size (140 characters). This was gathered using the twitter trickle. With domain related terms, like NFL, Tim Tebow, Jets.
    Approximately 9GB of tweets, yielding a 7GB inverted index.

    Blogs -- english language, small-medium document size, 10-100 sentences.
    30k+ from official NFL teams, going back several years.
    60k+ from ESPN as commentary going back several years.

    Play-by-plays -- semi-structured, repeated patterns with specific meaning.

	\item Measure of success

		We were able to create queries that gave interesting results.
		We consider this work a success.


  \item What are the main experimental results? Effectiveness/performance?

    Throughput for sentiment analysis is dominated by making a remote call.
    Throughput for POS tagging is dominated by making a python call (interpreted).

  \end{enumerate}

  \section{Conclusion}
  \begin{enumerate}\item What you have learnt through this project?

    Morgan - learned how to parse html, write and use a web crawler, 
		We learned conditional random fields stuff and how to setup and use
		the postgreql dbms.
    It is a difficult task to get and clean data.

		Christan - how to create complicated sql queries,
		creating full text indices and how to
		develop ajax calls to dynamic web site creation, and how to wrap almost 
		anything in a posgres UDF. 

		Joir-dan -- Databases are fickle, text analytics are pretty sweet and have
		a great upside.


  \end{enumerate}




\bibliographystyle{abbrv}
\bibliography{citation.bib}




\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
