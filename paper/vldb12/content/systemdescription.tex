
\section{System Description}

In this section we first discuss the general architecture the {\system} 
system. Next, we discuss the spirit and motivation behind the \system 
implementation of the text analysis algorithms.
Next, we discuss how \system excels with the difference classes of algorithms
implemented. 
Then we discuss how parallel database architectures improve query 
processing times.
Lastly, we propose our part-of-speech (POS) tagging implementation to 
give a practical example of a \system algorithm implementation.


\begin{figure}
   \begin{center}
        \includegraphics[scale=0.2]{content/graphics/arch.png}
        \caption{{\system} architecture}
        \label{fig:arch}
   \end{center}
\end{figure}
    
\subsection{System Architecture}

{\system} is a four layered system, as can be seen in Figure \ref{fig:arch}.
The user interface is where both naive and advanced users can construct queries over
text, structured data, and models. From the user interface, queries are then passed to the
DBMS, where both MADLib and {\system} libraries sit on top of the query processor to
add statistical and text processing functionality. 
It is important to emphasize that MADlib and {\system} perform functions
at the same logical layer.  These queries are processed using 
PostgreSQL and Greenplum's Parallel DB architecture to further optimize on 
replicated storage and parallel query optimization. 


%\subsection{{\system} Implementation}



% -------------------------------------------- Rewrite




% All new stuff-------------% -------------% ----------------
\subsection{MADden Implementation}
To enable text analytics, {\system} works alongside statistical
functions found in the MADlib library \cite{Cohen:2009:MSN:1687553.1687576}.
The implemented algorithms are developed to take advantage of the 
architecture and features of the DBMS.
The algorithms are first implemented in SQL or PL/pgSQL, as this language
exposes the execution pipeline to the query optimizer. 
SQL excels in first-order logic operations performed in batch.
If many loops and complicated data flows are involved, we use the PL/Python 
scripting language to control the execution.
We can use Python to make decisions to aid the query optimizer in algorithm
decision.
Dense or sparse array and matrix data structures can be represented
by specialized types. Operations on these structures can be performed in
SQL or calculated with calls to highly tuned numeric packages such as LAPACK
or custom c-UDFs.
During development of algorithms in MADden or MADLib we take these options 
into consideration.


\subsection{In-Database Algorithms}

In this section we describe the {\system} algorithms.
Many approaches exist for in-database information extraction. 
We build on our previous work using Conditional Random Fields (CRFs) for 
query-time information extraction \cite{wang2011hybrid}.
We use an hybrid approach to information extraction where we perform the 
extraction and the inference inside of the database. We rely on information
provided in the query to make decision on the type of algorithm used 
for extraction. 
We discuss this in detail in section \ref{sec:impdis}.

In {\system} we encounter situations requiring entity resolution problem.
Entity resolution or co-reference resolution is the problem where given any two
mentions of a name, the are clustered only if they refer to the same real 
world entity. This problem is difficult because you can have mentions that are
spelled the same but refer to different entities and conversely we different
spelling that may refer to the same entity.
Certain entities may be misrepresented by the presence of different names, 
misspellings in the text, or aliases. It is important to
resolve these entities appropriately to better understand the data. Increasingly
informal text, such as blog posts and tweets requires entity resolution. 
{\system} utilizes inverted indices within the database to perform text
analysis on documents. We can scan the inverted index of each 
document, filtering out documents that do not contain instances of the player names.
To handle misspellings and nicknames we use trigram indices to perform 
approximate matches of searches for names \cite{Jain:2009:BQO:1519103.1519108}.
\ceg{Explain why this is query driven technique and expplain the alternative}

Each text analysis function creates a view that can continue to be utilized.
Since views can be queried in the same manner as tables, we can join calculated
statistics with other tables, or other text analysis views, to construct more
interesting results. This, combined with expressing text analysis functionality
within a SQL query, allows for more complex queries to be realized. \\



\subsection{Parallelization}
With the ability to implement a parallel database architecture with %Postgres or 
Greenplum, we can utilize data parallelism to further optimize queries written
with {\system}. Each node within the parallel DB could run some query
over a subset of the data (data parallel). This includes the statistical methods
in MADLib, which were all built to be data parallel.

In POS tagging of a corpra is an integral task for many 
textual analytic operations. POS tagging can be performed per-paragraph or 
per-sentence meaning the process is easily parallelizeable in our queries.

With POS tagging performed we can perform the similarly parallel operation 
of sentiment analysis.
Sentiment analysis deals with the study of opinions expressed within text. 
The sentiment is usually expressed in terms of polarities, where positive
sentiment refers to the positive nature of the expressed opinion, and negative
nature for negative sentiment. Much work has already been done in this
area for document-level and entity-level sentiment \cite{o2010tweets,
zhang2011combining}. 
{\system} can apply sentiment analysis at document and sentence level.
\ceg{Kun, this is where Kun's section should go. We may want to move this back into the query driven algorithms section}


\subsection{Implementation Discussion}
\label{sec:impdis}

\eat{
%\noindent
%\textbf{Part-of-Speech Tagging}\\
\jag{This needs to be cleaned up a bit and some parts need to be distributed to
other sections.
I attempted to make the listings changes. And this is a general comment for the whole paper\ldots but we should
think about using the [strings]{underscore} package, considering all the
underscores we are escaping (Probably better comment for future papers\ldots)}


Core to many natural language processing(NLP) tasks, POS involves the
labeling of terms within text based on their function in a particular sentence.
MADden uses our own implementation of POS in Postgres and Greenplum. We are
committing our code to MADLib and it is in under review. The advantage of our
implementation lies in two aspects. The first one is that we extract features
for each distinct tokens instead of for every tokens.  It is the easy to imagine
the benefit it can bring when we are labeling a large corpus since the number of
distinct tokens is limited to the size of English vocabulary. We have a function
{\tt MADLIB\_SCHEMA.textfex\_generatemrtbl} to generate all the features. This
function produces two factors tables: a {\tt rfactor} table, which store single
state features, e.g., \textit{RegexFeature} and a {\tt mfactor} table to store
the two state features such as \textit{EdgeFeature}.  The second advantage is
that we use SQL statements as the control flow driver which is inherently
parallel for POS tagging. And the computation intensive functions are
implemented in C. The following SQL statements is for geting the best POS label 
sequence for all sentences stored in segtbl. For each sentence,
{\tt MADLIB\_SCHEMA}.\textit{\_\_vcrf\_top1\_label} implemented in
C is invoked to get the best label sequence.


%\lstset{breaklines=true}
%\lstset{tabsize=2}
%\lstset{basicstyle=\small}
%\begin{lstlisting}{language=SQL}
\begin{small}
\begin{alltt}
INSERT INTO resulttbl

SELECT segtbl.doc\_id, 
       MADLIB\_SCHEMA.\_\_vcrf\_top1\_label(segtbl.seglist,
                                            mfactor.score, rfactor.score)
FROM   segtbl, mfactor, rfactor;

\end{alltt}
\end{small}
%\end{lstlisting}
}


%%%---------new pos section

Core to many natural language processing(NLP) tasks, POS involves the
labeling of terms within text based on their function in a particular sentence.
\system uses our own implementation of POS in Postgres and Greenplum. We are
committing our code to MADLib and it is in under review.

\system uses first order chain conditional random field to model the labeling
of a sequence of tokens. The factor graph has observed nodes on each sentence
token, latent label variables attached to each token.
Factors are only associated with one node at the beginning and end of the chain.
All other factors are pair wise. 

Training the tagger is a one time task that is performed outside the 
DBMS and stored in XXX tables. \ceg{Add some detail on how training relates.}

In the linear chain CRF model, factors are connected one or two nodes.  
We generate the features using a function YYY. \ceg{I don't think we need the
actual name for the Viterbi method, we can use a fake more clean one for 
readability}
This function produces a table
\textit{rfactor} for single state features and a table \textit{mfactor} for two
state features.

Inference is performed over the stored models in order to find the highest 
assignment of labels in the model.
We use develop a Viterbi top1 implementation, this method works by 
\ceg{Add a quick description of how viteri works.}

We use the python language to manage the work flow execution of feature 
generation and inference.
The computationally expensive functions viterbi are implemented as c-UDFs.
The execution across a dataset is implemented in SQL, in GreenPlum the query
is executed in parallel.


Implementing POS tagging inside the DBMS allows us to perform inference over a 
subset of tokens response to a query instead of performing batch tagging over 
all tokens.
We also get the benefit of using the query engine to parallelize our queries
without losing the ability to drive the work flow using PL/Python.


\ceg{Example execution call of the pos function for a subset of data maybe
perform POS tagging only for sentences that contain 'Jaguars'.}



%-----------end new pos section

