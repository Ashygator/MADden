
\section{System Description}

We discuss the architecture, functionalities, and the implementation of
the {\system} system.
\begin{figure}
   \begin{center}
        \includegraphics[scale=0.2]{content/graphics/arch.png}
        \caption{{\system} architecture}
        \label{fig:arch}
   \end{center}
\end{figure}
    
\subsection{System Architecture}

{\system} is a four layered system, as can be seen in Figure \ref{fig:arch}.
The user interface is where both naive and advanced users can construct queries over
text, structured data, and models. From the user interface, queries are then passed to the
DBMS, where both MADLib and {\system} libraries sit on top of the query processor to
add statistical and text processing functionality. 
It is important to emphasize that MADlib and {\system} perform functions
at the same logical layer.  These queries are processed using 
PostgreSQL and Greenplum's Parallel DB architecture to further optimize on 
replicated storage. 


%\subsection{{\system} Implementation}

In this section we discuss the spirit behind the \system implementation
of the text analysis algorithms. 
Next, we discuss the how we use user queries and the DBMS to in 
information extraction, entity resolution and part-of-speech (POS) tagging.
Finally, we discuss how parallel database architectures improve query 
processing times.


% -------------------------------------------- Rewrite




% All new stuff-------------% -------------% ----------------
\subsection{MADden Implementation}
To enable text analytics, {\system} works alongside statistical
functions found in the MADlib library \cite{Cohen:2009:MSN:1687553.1687576}.
The implemented algorithms are developed to take advantage of the 
architecture and features of the DBMS.
The algorithms are first implemented in SQL or PL/pgSQL, as this language
exposes the execution pipeline to the query optimizer. 
SQL excels in first-order logic operations performed in batch.
If many loops and complicated data flows are involved, we use the PL/Python 
scripting language to control the execution.
We can use Python to make decisions to aid the query optimizer in algorithm
decision.
Dense or sparse array and matrix data structures can be represented
by specialized types. Operations on these structures can be performed in
SQL or calculated with calls to highly tuned numeric packages such as LAPACK
or custom c-UDFs.
During development of algorithms in MADden or MADLib we take these options 
into consideration.


\subsection{Query-driven Algorithms}


Many approaches exist for in-database information extraction. 
We build on our previous work using Conditional Random Fields (CRFs) for 
query-time information extraction \cite{wang2011hybrid}.
\ceg{Here better describe the information extraction task}
We use an hybrid approach to information extraction where we perform the 
extraction and the inference inside of the database. We rely on information
provided in the query to make decision on the type of algorithm used 
for extraction. For example, we use viterbi if ... o.w. we use.. \ceg{TODO fix this to better explain why query driven is good}

In {\system} we encounter situations requiring entity resolution problem.
Entity resolution or co-reference resolution is the problem where given any two
mentions of a name, the are clustered only if they refer to the same real 
world entity. This problem is difficult because you can have mentions that are
spelled the same but refer to different entities and conversely we different
spelling that may refer to the same entity.
Certain entities may be misrepresented by the presence of different names, 
misspellings in the text, or aliases. It is important to
resolve these entities appropriately to better understand the data. Increasingly
informal text, such as blog posts and tweets requires entity resolution. 
{\system} utilizes inverted indices within the database to perform text
analysis on documents. We can scan the inverted index of each 
document, filtering out documents that do not contain instances of the player names.
To handle misspellings and nicknames we use trigram indices to perform 
approximate matches of searches for names \cite{Jain:2009:BQO:1519103.1519108}.
\ceg{Explain why this is query driven technique and expplain the alternative}

Each text analysis function creates a view that can continue to be utilized.
Since views can be queried in the same manner as tables, we can join calculated
statistics with other tables, or other text analysis views, to construct more
interesting results. This, combined with expressing text analysis functionality
within a SQL query, allows for more complex queries to be realized. \\



\subsection{Parallelization}
With the ability to implement a parallel database architecture with %Postgres or 
Greenplum, we can utilize data parallelism to further optimize queries written
with {\system}. Each node within the parallel DB could run some query
over a subset of the data (data parallel). This includes the statistical methods
in MADLib, which were all built to be data parallel.

In part-of-speech (POS) tagging of a corpra is an integral task for many 
textual analytic operations. POS tagging can be performed per-paragraph or 
per-sentence meaning the process is easily parallelizeable in our queries.

With POS tagging performed we can perform the similarly parallel operation 
of sentiment analysis.
Sentiment analysis deals with the study of opinions expressed within text. 
The sentiment is usually expressed in terms of polarities, where positive
sentiment refers to the positive nature of the expressed opinion, and negative
nature for negative sentiment. Much work has already been done in this
area for document-level and entity-level sentiment \cite{o2010tweets,
zhang2011combining}. 
{\system} can apply sentiment analysis at document and sentence level.
\ceg{Kun, this is where Kun's section should go. We may want to move this back into the query driven algorithms section}


\noindent
\textbf{Part-of-Speech Tagging}\\

Core to many natural language processing(NLP) tasks, POS involves the
labeling of
terms within text based on their function in a particular sentence. MADden
uses our
own implementation of POS in Postgres and Greenplum. We are committing our
code to
MADLib and it is in under review. The advantage of our implementation lies
in two
aspects. The first one is that we extract features for each distinct
tokens instead of
for every tokens.  It is the easy to imagine the benefit it can bring when
we are
labeling a large corpus since the number of distinct tokens is limited to
the size
of English vocabulary. We have a function
'MADLIB\_SCHEMA.textfex\_generatemrtbl' to
generate all the features. This function produces two factors tables: a
'rfactor'
table, which store single state features, e.g., 'RegexFeature' and a
'mfactor'
table to store the two state features such as 'EdgeFeature'.  The second
advantage is
that we use SQL statements as the control flow driver which is inherently
parallel for POS tagging. And the computation intensive functions are
implemented in C. The
following SQL statements is for geting the best POS label sequence for all
sentences
stored in segtbl.  For each sentence,
'MADLIB\_SCHEMA.\_\_vcrf\_top1\_label' implemented in
C is invoked to get the best label sequence.

INSERT INTO resulttbl

SELECT   segtbl.doc\_id, MADLIB\_SCHEMA.\_\_vcrf\_top1\_label(segtbl.seglist,
        mfactor.score, rfactor.score)

FROM   segtbl, mfactor, rfactor;
